{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pclass        False\n",
       "Name          False\n",
       "Sex           False\n",
       "Age           False\n",
       "SibSp         False\n",
       "Parch         False\n",
       "Ticket        False\n",
       "Fare          False\n",
       "Cabin          True\n",
       "Embarked      False\n",
       "SizeFamily    False\n",
       "Solo          False\n",
       "dtype: bool"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def genderResult(sex):\n",
    "    if (sex == 'male'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def embarkedResult(embarked):\n",
    "    if (embarked == 'S'):\n",
    "        return 2\n",
    "    elif (embarked == 'Q'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def nameResult(name):\n",
    "    if ((name.find('Jonkheer') or name.find('Don.') or name.find('Sir') or name.find('Countess') or name.find('Lady')) > -1):\n",
    "        return 6\n",
    "    if ((name.find('Capt') or name.find('Col') or name.find('Major') or name.find('Dr') or name.find('Rev')) > -1):\n",
    "        return 5\n",
    "    if ((name.find('Mrs') or name.find('Ms') or name.find('Mme')) > -1):\n",
    "        return 4\n",
    "    elif (name.find('Mr') > -1):\n",
    "        return 3\n",
    "    elif (name.find('Master') > -1):\n",
    "        return 2\n",
    "    elif ((name.find('Miss') or name.find('Mlle')) > -1):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "def ageResult(age):\n",
    "    if (embarked == 'S'):\n",
    "        return 2\n",
    "    elif (embarked == 'Q'):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df_train = pd.read_csv(\"train.csv\", index_col=0)\n",
    "df_train['Sex'] = df_train['Sex'].apply(genderResult)\n",
    "df_train['Embarked'] = df_train['Embarked'].apply(embarkedResult)\n",
    "df_train['Name'] = df_train['Name'].apply(nameResult)\n",
    "df_train['SizeFamily'] = 1 + df_train['SibSp'] + df_train['Parch']\n",
    "df_train['Solo'] = [1 if x == 1 else 0 for x in df_train['SizeFamily']]\n",
    "\n",
    "median_age_men=df_train[df_train['Sex']==1]['Age'].median()\n",
    "median_age_women=df_train[df_train['Sex']==0]['Age'].median()\n",
    "df_train['Age'] = [median_age_men if x ==1 else median_age_women for x in df_train['Sex']]\n",
    "\n",
    "mean_embarked_s=df_train[df_train['Embarked']==2]['Fare'].mean()\n",
    "mean_embarked_q=df_train[df_train['Embarked']==1]['Fare'].mean()\n",
    "mean_embarked_c=df_train[df_train['Embarked']==0]['Fare'].mean()\n",
    "mean_fare = df_train[\"Fare\"].mean()\n",
    "mean_fare = int(mean_fare)\n",
    "df_train[\"Fare\"] = df_train[\"Fare\"].fillna(mean_fare)\n",
    "#df_train['Fare'] = [mean_embarked_s if x ==2 else mean_embarked_q if x ==1 else mean_embarked_c for x in df_train['Embarked']]\n",
    "\n",
    "\n",
    "X_train = df_train.drop(['Survived','Ticket','Cabin','SibSp','Parch','Age','Name','SizeFamily'], axis=1)\n",
    "y_train = df_train['Survived']\n",
    "\n",
    "df_test = pd.read_csv(\"test.csv\", index_col=0)\n",
    "df_test['Sex'] = df_test['Sex'].apply(genderResult)\n",
    "df_test['Embarked'] = df_test['Embarked'].apply(embarkedResult)\n",
    "df_test['Name'] = df_test['Name'].apply(nameResult)\n",
    "df_test['SizeFamily'] = 1 + df_test['SibSp'] + df_test['Parch']\n",
    "df_test['Solo'] = [1 if x == 1 else 0 for x in df_test['SizeFamily']]\n",
    "median_age_men=df_test[df_test['Sex']==1]['Age'].median()\n",
    "median_age_women=df_test[df_test['Sex']==0]['Age'].median()\n",
    "df_test['Age'] = [median_age_men if x ==1 else median_age_women for x in df_test['Sex']]\n",
    "mean_fare = df_test[\"Fare\"].mean()\n",
    "mean_fare = int(mean_fare)\n",
    "mean_embarked_s=df_test[df_test['Embarked']==2]['Fare'].mean()\n",
    "mean_embarked_q=df_test[df_test['Embarked']==1]['Fare'].mean()\n",
    "mean_embarked_c=df_test[df_test['Embarked']==0]['Fare'].mean()\n",
    "#df_test['Fare'] = [mean_embarked_s if x ==2 else mean_embarked_q if x ==1 else mean_embarked_c for x in df_test['Embarked']]\n",
    "\n",
    "df_test[\"Fare\"] = df_test[\"Fare\"].fillna(mean_fare)\n",
    "X_test = df_test.drop(['Ticket','Cabin','SibSp','Parch','Age','Name','SizeFamily'], axis=1)\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "\n",
    "# print out columns with nan\n",
    "df_test.isna().any()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeSubmissionFile(y_pred):\n",
    "    fh = open(\"submission.csv\", \"w\")\n",
    "    fh.write(\"PassengerId,Survived\\n\")\n",
    "    for i in range (len(y_pred)):\n",
    "        pid = 892 + i\n",
    "        fh.write(str(pid))\n",
    "        fh.write(',')\n",
    "        fh.write(str(y_pred[i]))\n",
    "        fh.write('\\n')\n",
    "    fh.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid search config: {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 400}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = { \n",
    "    'n_estimators': [50, 300, 400, 500],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [2,4,5,6,7,8,10],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "gridcv = GridSearchCV(RandomForestClassifier(random_state=10), param_grid=param_grid, cv=5, n_jobs=-1)\n",
    "gridcv.fit(X_train, y_train)\n",
    "print('grid search config: %s' % gridcv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=400, criterion='entropy', max_depth=4, max_features='auto')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "makeSubmissionFile(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'friedman_mse', 'learning_rate': 0.075, 'max_depth': 4, 'min_samples_leaf': 0.13636363636363638, 'min_samples_split': 0.31818181818181823, 'n_estimators': 100}\n",
      "0.8092259675405742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075],\n",
    "    \"max_depth\":[2,4],\n",
    "    \"n_estimators\":[10,50,100],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 12),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 12)\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(GradientBoostingClassifier(random_state=10), params, cv=10, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient = GradientBoostingClassifier(criterion='friedman_mse', n_estimators=100, max_depth=4, learning_rate=0.025, min_samples_leaf=0.1, min_samples_split=0.1)\n",
    "gradient.fit(X_train, y_train)\n",
    "y_pred = gradient.predict(X_test)\n",
    "makeSubmissionFile(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.2, 'n_estimators': 300}\n",
      "0.8204619225967541\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "params = {\n",
    "    \"learning_rate\": [0.01, 0.025, 0.05, 0.075, 0.1, 0.15, 0.2],\n",
    "    \"n_estimators\":[10,50,100,150, 300]\n",
    "    }\n",
    "\n",
    "clf = GridSearchCV(AdaBoostClassifier(random_state=10), params, cv=10, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostClassifier(n_estimators=300, learning_rate=0.2)\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred = ada.predict(X_test)\n",
    "makeSubmissionFile(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel = 'rbf', random_state = 0)\n",
    "svc.fit(X_train, y_train)\n",
    "y_pred = svc.predict(X_test)\n",
    "makeSubmissionFile(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression(penalty=\"l1\", solver='liblinear')\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "makeSubmissionFile(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'depth': 3, 'iterations': 100, 'l2_leaf_reg': 1e-20, 'leaf_estimation_iterations': 5, 'logging_level': 'Silent', 'loss_function': 'Logloss'}\n",
      "0.7868039950062422\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "params = {'iterations': [100,200,300,500,600,700,800],\n",
    "          'depth': [2,3, 4, 5, 6],\n",
    "          'loss_function': ['Logloss', 'CrossEntropy'],\n",
    "          'l2_leaf_reg': np.logspace(-20, -19, 3),\n",
    "          'leaf_estimation_iterations': [5, 10, 15],\n",
    "          'logging_level':['Silent']\n",
    "         }\n",
    "clf = GridSearchCV(CatBoostClassifier(), params, cv=10, n_jobs=-1, scoring=\"accuracy\")\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = CatBoostClassifier(depth = 2, iterations = 300, l2_leaf_reg = 1e-20, leaf_estimation_iterations = 10, logging_level = 'Silent', loss_function = 'Logloss', random_seed=42)\n",
    "cat.fit(X_train, y_train)\n",
    "y_pred = cat.predict(X_test)\n",
    "makeSubmissionFile(y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/madhumithav/opt/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:07:36] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "{'colsample_bytree': 0.75, 'learning_rate': 0.01, 'max_depth': 4, 'min_child_weight': 1, 'n_estimators': 100, 'subsample': 1}\n",
      "0.8260372857949909\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "params     = {\"subsample\":[0.75, 1],\n",
    "              \"colsample_bytree\":[0.75, 1],\n",
    "              \"max_depth\":[4, 5, 6],\n",
    "              \"min_child_weight\":[1, 5],\n",
    "              \"n_estimators\":[100,300],\n",
    "              \"learning_rate\": [0.01, 0.05, 0.1]}\n",
    "clf = GridSearchCV(XGBClassifier(), params, cv=5, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)\n",
    "print(clf.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14:51:51] WARNING: /opt/concourse/worker/volumes/live/7a2b9f41-3287-451b-6691-43e9a6c0910f/volume/xgboost-split_1619728204606/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier(colsample_bytree = 1, learning_rate = 0.01, max_depth = 6, min_child_weight = 1, n_estimators = 100, subsample = 1)\n",
    "xgb.fit(X_train, y_train)\n",
    "y_pred = xgb.predict(X_test)\n",
    "makeSubmissionFile(y_pred)\n",
    "                    \n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 6}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "params = {'criterion':['gini','entropy'],\n",
    "          'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n",
    "\n",
    "clf = GridSearchCV(DecisionTreeClassifier(), params, cv=5, n_jobs=-1)\n",
    "clf.fit(X_train, y_train)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_tree = DecisionTreeClassifier(criterion = 'entropy', max_depth=6)\n",
    "decision_tree.fit(X_train, y_train)\n",
    "y_pred = decision_tree.predict(X_test)\n",
    "makeSubmissionFile(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Survived      1.000000\n",
      "Pclass        0.338481\n",
      "Name          0.159986\n",
      "Sex           0.543351\n",
      "Age           0.543351\n",
      "SibSp         0.035322\n",
      "Parch         0.081629\n",
      "Fare          0.257307\n",
      "Embarked      0.174199\n",
      "SizeFamily    0.016639\n",
      "Solo          0.203367\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "correlation = df_train.corr()\n",
    "correlation_target = abs(correlation[\"Survived\"])\n",
    "print(correlation_target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
